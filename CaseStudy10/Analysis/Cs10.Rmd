---
title: 'Team CCAD Case Study 10: Missing Data Imputation'
author: "David Josephs, Andy Heroy, Carson Drake, Che' Cobb"
date: '`r Sys.Date()`'
output:
  html_document:
    df_print: paged
    fig_caption: true
    fig_height: 10
    fig_retina: yes
    fig_width: 10
    keep_md: yes
    number_sections: yes
    theme: readable
    toc: yes
  pdf_document:
    toc: yes
---


```{r setup, include = F}
knitr::opts_chunk$set(warning = F, message = F, dev = "svg", fig.align = "center")
#knitr::opts_chunk$set(tidy = T)
knitr::opts_chunk$set(comment = '#>')
options(digits = 2)

```

# Introduction

As aspiring data scientists, the first question we must always ask when given a task is: "Do we have the right data?" and "Is our data clean?". We cannot really help it if the data is just not there (we don't have any useful data), but we can help the second question. If data is missing, we must come up with a strategy to impute it, or to replace the missing variables with some other sort of information. In this study, we will demonstrate the utility of such methods, and when they are effective vs when they are not. First, lets set up the environment for this study, importing necessary libraries and other formatting tools:

```{r, echo = FALSE}
# allows us to run our stuff in python
library(reticulate)
# this set of functions allows us to autoincrement our figures
counter <- function() {
  x <- 0
  return (
          function() {
            # Assigning outside of scope! The real purpose of <-
            x <<- x+1
            return(x)
          }

  )
}

# initialize a new counter
cnt <- counter()

# call the counter with a bolded figure caption
cap <- function(str) {
  paste("**Figure", cnt(), ":**",str)
}
```

```{python}
from sklearn.datasets import load_boston
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import Imputer
from sklearn.base import BaseEstimator
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from typing import Callable
from sklearn.impute import SimpleImputer
from pprint import pprint
import pandas as pd
plt.style.use('bmh')
```

# Analysis

## Initial Analysis of the Data

First, we will use pandas to perform a cursory analysis of the data, as this is always good practice:

```{python, fig.cap=cap("**Figure 1**: The columns each seem to have a different distribution. Other than RM, they either have a strong skew or an almost bimodal distribution.")}
boston = load_boston()
col_names = boston['feature_names']
bos = pd.DataFrame(data = boston['data'], columns = boston['feature_names'])
bos_target = boston['target']

#Check missing values. 
bos.isnull().sum()

_ = bos.hist(bins=50, figsize = (20,15))
plt.show()
```

Looks like we have no missing data to start with, so we will need to create some. This plot does suggest some outliers. We can check those in the following manner:

```{python}
for k, v in bos.items():
	q1 = v.quantile(0.25)
	q3 = v.quantile(0.75)
	irq = q3 - q1
	v_col = v[(v <= q1 - 1.5 * irq) | (v >= q3 + 1.5 * irq)]
	perc = np.shape(v_col)[0] * 100.0 / np.shape(bos)[0]
	print("%s outliers = %8.2f%%" % (k, perc))
```


Now that we have inspected our data and have soothed our cautious souls, we can go ahead and proceed with our analysis.

## Building a Baseline Model.

In general, it is always best to build a simple baseline model first, before we proceed with our analysis. In the case of our missing data analysis, we will want to know the goodness of fit ($R^2$) and the loss (mean squared error) of our models. The same is true for the baseline. Let us first define a utility function for fitting and getting scores of a model:

```{python}
X, y = load_boston(return_X_y=True)
# default parameters to use
parameter_dict = {'n_jobs':-1}
# dictionary for results
results_dict = {}


def get_scores(features: np.ndarray,
               targets: np.ndarray,
               model_class: BaseEstimator=LinearRegression,
               pars: dict=parameter_dict) -> dict:
    # initialize model using external pars
    model = model_class(**pars)
    model.fit(features, targets)
    preds = model.predict(features)
    # get the scores
    goodness = r2_score(y, preds)
    loss = mean_squared_error(y, preds)
    return {'goodness_of_fit':goodness, 'mse':loss}
    

pprint(get_scores(X,y))
```

We have a goodness of fit of 0.74 and a loss of 21.89. 

## Missing at Random

Next we will proceed to compare different missing rates and imputation methods. First, lets define a function which replaces a random sample of a random columns values with NaNs:

```{python}
def random_miss(data: np.ndarray, prop: int) -> np.ndarray:
    # raise an error if our proportion is less than 0 or more than 100
    if type(prop) is not int or prop <= 0 or prop >=100:
        raise ValueError('needs to be an int less than 100 and greater than zero!')
    # sample columns, sample rows
    nrows = data.shape[0]
    idy = np.random.choice(data.shape[-1], 1)
    idx = np.random.choice(nrows, int(nrows*prop/100), replace=False)
    out = data.copy()
    # turn sample into nas
    out[idx, idy] = np.nan
    return out
```

Now, we need to be sure of ourselves, since this is random, so we will define two more functions: one which repeatedly makes columns missing at random and fits an imputer, and one which takes a list of scores and returns the mean and standard deviation of that list:

```{python}
def _stats_dict(arr: list) -> dict:
    return {'avg': np.mean(arr), 'std': np.std(arr)}

def iterate_stats(features: np.ndarray,
                  targets: np.ndarray,
                  impute_method: str=None,
                  prop: int=0,
                  iters: int=10,
                  model_class: BaseEstimator=LinearRegression,
                  pars: dict=parameter_dict) -> dict:
    r2 = []
    mse = []
    for _ in range(iters):
        # because we raise that valueerror in random_miss. This allows us to iterate our baseline too
        if prop ==  0:
            data = features
        else:
            tmp = random_miss(features, prop)
            data = SimpleImputer(strategy=impute_method).fit_transform(tmp)
            del tmp
        res = get_scores(data, targets, model_class, pars)
        # append scores to list
        r2 += [res['goodness_of_fit']]
        mse += [res['mse']]
    return {'goodness_of_fit': _stats_dict(r2), 'loss': _stats_dict(mse)}
```

With all this defined, we can go ahead and build our dictionary of results:

```{python}
# baseline model
results_dict['baseline'] = iterate_stats(X,y)

# define iterables
impute_types = ['mean','median']
miss_props = [1, 5, 10, 20, 33, 50]
n_rounds = X.shape[-1]*100

for imp in impute_types:
    results_dict[imp] = {p: iterate_stats(X, y, prop=p, iters=n_rounds, impute_method=imp) for p in miss_props}
pprint(results_dict)
```

This ultra nested dict is a bit hard to digest. Next, we will define two more functions: the first will take in a nested dictionary (the results at an imputation method), and get the average and one standard deviation upper and lower bounds of the results. The second will allow us to quickly plot our data:

```{python}
def results_to_series(res: dict, f_type: str) -> dict:
    upper = []
    lower = []
    avg = []
    x = list(res.keys())
    avg = [res[k][f_type]['avg'] for k in x]
    std = [res[k][f_type]['std'] for k in x]
    # for plotting
    # lower and upper bounds for lovely error on the line plot
    upper = [avg[i] + std[i] for i in range(len(avg))]
    lower = [avg[i] - std[i] for i in range(len(avg))]
    return {'x':x, 'y':avg, 'upper': upper, 'lower':lower}


def make_plotter(f_type: str) -> Callable:
    # returns a plotting function to either plot the goodness or the loss
    def plot_generic_stats(results: dict=results_dict)-> None:
        fig = plt.figure(figsize=(12,10))
        ax = plt.subplot()
        base = results_dict['baseline'][f_type]
        # horizontal line for baseline
        ax.axhline(base['avg'], color='r', xmin=0, xmax=50, label='Baseline')
        keys = [k for k in results.keys() if k != 'baseline']
        for k in keys:
            tmp = results_to_series(results[k], f_type)
            ax.plot(tmp['x'], tmp['y'], label=k.title())
            ax.fill_between(tmp['x'], tmp['lower'], tmp['upper'], alpha=0.1, label=f"{k} error".title())
        ax.legend()
        ax.set_title(f"{f_type} vs Percentage of Missing Data, by Imputation Method".title().replace("_"," "))
        ax.set_xlabel('Percentage of Missing Data')
        ax.set_ylabel(f_type.title().replace("_"," "))
        plt.show()
        plt.clf()

    return plot_generic_stats


plot_goodness = make_plotter('goodness_of_fit')
plot_loss = make_plotter('loss')
```

Now that we have that out of the way, we can go ahead and plot our results. First, lets check out goodness of fit:

```{python, fig.cap = "**Figure 2**: Mean and Median imputation performed almost identically, and neither was much much worse than the baseline."}
plot_goodness()
```

As the amount of missing data grew, the goodness of fit went down (but not by a lot). However, at low missing rates, this did absolutely fine. That means for simple datasets with only a bit of missing data, its likely we can quickly just throw in a simple imputation method and get near the same results as if our data were all there. Lets double check this is the case with loss:

```{python,  fig.cap = "**Figure  3**: Mean and Median imputation again performed almost the same. This supports our hypothsis"}
plot_loss()
```








