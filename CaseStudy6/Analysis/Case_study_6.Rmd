---
title: 'Team ACDC Case Study 6: Spam Ham, Problem 19'
author: "David Josephs, Andy Heroy, Carson Drake, Che' Cobb"
date: '`r Sys.Date()`'
output:
  html_document:
    code_folding: hide
    df_print: paged
    fig_caption: yes
    fig_height: 10
    fig_retina: yes
    fig_width: 10
    highlight: haddock
    keep_md: yes
    number_sections: yes
    theme: readable
    toc: yes
  pdf_document:
    toc: yes
---


```{r setup, include = F}
knitr::opts_chunk$set(warning = F, message = F, dev = "svg", fig.align = "center")
knitr::opts_chunk$set(tidy = T)
knitr::opts_chunk$set(comment = '#>')
options(digits = 2)

```

```{r Load_Libraries}
library(rpart)
library(caret)
library(dplyr)
library(rpart.plot)

```

# Introduction

Spam email is an ever present annoyance in most of modern day life. This case study is to examine the Spam Assassin dataset (http://spamassassin.apache.org) in which we evaluate 9000 emails with spam filters by classification tree's and recursive partitioning.  To do this, the libararies rpart is utilized and we attempt to improve upon default model classification accuracies by exploring the many parameters available in the rpart.control function.  


we've selected question 19 from the Nolan and Lang book for our analysis and exploration. The question states: 

Consider the other parameters that can be used to control the recursive partitioning process. Read the documentation for them in the rpart.control() documentation. Also, carry out an Internet search for more information on how to tweak the rpart() tuning parameters. Experiment with values for these parameters. Do the trees that result make sense with your understanding of how the parameters are used? Can you improve the prediction using them?

# Background

The spam assassin dataset is contrived of 9000 emails each comprised of a header, message body and attachment.  The first goal is to understand the design of the email structure in order to correctly indentify what portion of the email is the header, message body and attachments.  After dropping attachments, the headers and message bodies are processed for characteristics that possibly could mark an email as spam.  Looking for characteristics such as capitolization, punctuation or word counts of known spam words that can be found within.  

## Data Description

# Note.  Could just make a giant r_script with all the functions and load it into here.


```{r dataload}
# we are starting from here, 
# the code used to create the RDA will be shown in the appendix, 
# it seems to run fine on windows but crash on linux
# so this was a good compromise without digging too deeply into the web of 
# lapplys

load("../Data/data.Rda")
emails <- emailDFrp

setupRnum = function(data) {
  logicalVars = which(sapply(data, is.logical))
  facVars = lapply(data[ , logicalVars], 
                   function(x) {
                      x = as.numeric(x)
                   })
  cbind(facVars, data[ , - logicalVars])
}

emails = setupRnum(emails)
dim(emails)
emails[is.na(emails)]<-0

df_process <- emails

```

# Analysis

To start off the analysis, we've used the methods provided in Nolan and Lang [1] for importing a clean dataset that can be parsed for examples of spam and the generated features about each email message.  We're to answer the question of exploring the rpart.control function so we will import the generated dataset from the starter RMD that was provided by Dr. Slater.  See the appendix for the code that was associated with it.  

Our next step is to explore the recursive partitioning algotithm to pass it various parameters that may improve its classification accuracy.  The parameters that we're going to explore are listed below.  

```{r parmtable, fig.cap = "Data Cleaning Summary"}
pander::pander(
 list(
    cp-complexity ="This parameter is meant to improve computing efficiency in order to avoid uncessary splits if there isn't an increase in R-squared value.  Our experiment will evaluate cp at the following intervals: .00001, .0001, .005, and 0.01",
    minsplit = "Seeing as a tree is the result of the rpart classification.  The minesplit parameter defintes the number of observations necessary in an a parent node in order to quality that branch to be split.  We chose to 3:1 ratio of minsplit to minbucket.",
    minbucket = "This parameter controls the lowest number of observations that can exist in an end/terminal node.",
    maxdepth = "This parameter defines how deep a final tree can be built by limiting the amount of levels of nodes that can be created.  The help files suggest not going too deep in a tree as it will result in overfitting."

    ))
```




## Default R part Analysis

```{r}

set.seed(54)

#Get a count for spam and ham
is_Spam_ct <- table(df_process$isSpam)["T"]
is_Ham_ct <- table(df_process$isSpam)["F"]

#Sample the indexes
testSpamidx = sample(is_Spam_ct, size = floor(is_Spam_ct/3))
testHamidx = sample(is_Ham_ct, size = floor(is_Ham_ct/3))

#Create DF for test and train
df_test_rpart = rbind(df_process[df_process$isSpam == "T",][testSpamidx,],
                      df_process[df_process$isSpam == "F",][testHamidx, ])

df_train_rpart = rbind(df_process[df_process$isSpam == "T",][-testSpamidx,],
                      df_process[df_process$isSpam == "F",][-testHamidx, ])

dim(df_train_rpart)

#Fitting the regression tree.
rpart_fit = rpart(isSpam ~ ., 
                  data = df_train_rpart,
                  minsplit = 2,
                  method = "class")

#Plot it with either prp or rpart.plot
#prp(rpart_fit, extra = 106)
rpart.plot(rpart_fit)

#Now lets make some predictions.  We'll predict on the df_test_rpart, with the fit we just achieved directly above. 

rp_predict <- predict(rpart_fit,
                      newdata = df_test_rpart[, names(df_test_rpart) != "isSpam"],
                      type = "class")

#summary(rp_predict)

spam_predict = rp_predict[ df_test_rpart$isSpam == "T"]
ham_predict = rp_predict[ df_test_rpart$isSpam == "F"]

sum(spam_predict == "T")/length(spam_predict)
sum(ham_predict == "T")/length(ham_predict)

```


## Parameter Search
```{r paramsearch}

#Setting up parameters for the rpart.control gridsearch

cp <- c(cp=c(0.00001,.0001,.005,.001))

max_depth <- c(max_depth = round(seq(3,30,length=10),0))

min_split <- c(min_split = round(seq(2, 100, length=30),0) + (round(seq(2, 100, length=30),0)%%2))

min_bucket <-c(min_split/2)

params <- expand.grid(cp=cp, max_depth=max_depth, min_split=min_split, min_bucket=min_bucket)

#combo_count <- nrow(params)

#Possibly sample these because its 36000 combinations
param_sample = params[sample(nrow(params),2400),]


```


```{r paramfunc}


````


Minsplit vs ntrees and accuracy/related metrics

## Ratio of minsplit and minbuxkets
normally its 3:1, see what happens

## Complexity and interaction with minsplit

### Lots o trees

### Few o trees

## Maxdepth

### Deep and wide

### Deep and narrow

### Short and wide

### Short and narrow

## Surragets
maybe
##xval
maybe

# Conclusion

Use cases for interesitng chouces

## References
[1] D. Lang and D. Nolan, Data Science in R: A Case Studies Approach to Computation Reasoning and Problem Solving. New York, New York: CRC Press. 

